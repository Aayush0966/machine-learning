{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5694f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [1, 1, 0, 0, 1, 1],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 0, 1, 1],\n",
    "    [0, 0, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 1],\n",
    "    [0, 1, 1, 0, 1, 0],\n",
    "    [1, 1, 0, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 1, 0],\n",
    "    [1, 0, 1, 1, 0, 1],\n",
    "    [0, 1, 0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "y = np.array([1,0,1,0,1,0,1,0,1,0,1,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a90fe1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Computes the entropy of the labels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: np.ndarray \n",
    "        Array of labels. eg: [1,0,0,1,1,1,0]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Entropy value which gives the impurity of the labels\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "        - Uses the formula: H(y) = -sum(p_i * log2(p_u)) for all classes of i.\n",
    "        - Zero probs are ignored to avoid log2(0).\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Input: y = [1, 0, 1, 0, 1]\n",
    "    Step 1: Calculate probabilities\n",
    "        - Class 0: count = 2, probability = 2/5 = 0.4\n",
    "        - Class 1: count = 3, probability = 3/5 = 0.6\n",
    "    Step 2: Apply entropy formula\n",
    "        - H(y) = -(0.4 * log2(0.4) + 0.6 * log2(0.6))\n",
    "        - H(y) = -(0.4 * (-1.32) + 0.6 * (-0.74))\n",
    "        - H(y) = -(-0.528 + (-0.444)) = 0.972\n",
    "    Output: 0.972 (high entropy indicating mixed classes)\n",
    "    \"\"\"\n",
    "    probs = np.bincount(y) / len(y) # bincount counts how many samples are there in each class\n",
    "    probs = probs[probs > 0] # removing zero probs\n",
    "    return -np.sum(probs * np.log2(probs)) # applying the formula\n",
    "\n",
    "\n",
    "def info_gain(x, y):\n",
    "    \"\"\"\n",
    "    Calculates the information gain of splitting labels y using feature x.\n",
    "    \n",
    "    :param x: np.ndarray\n",
    "            Array of single feature's values (shape: n_sampels, ).\n",
    "\n",
    "    :param y: np.ndarray\n",
    "            Array of target labels (shape: n_samples, ).\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    float   \n",
    "        Info gain obtained by splitting y based on feature x\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "        - Info gain = parent entropy - weighted child entropy\n",
    "        - Weighted child entropy is calculated based on the proportion of samples\n",
    "      in each unique feature value.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Input: x = [1, 0, 1, 0, 1], y = [1, 0, 1, 0, 1]\n",
    "    Step 1: Calculate parent entropy\n",
    "        - Parent entropy = entropy([1, 0, 1, 0, 1]) = 0.972\n",
    "    Step 2: Split by feature values\n",
    "        - When x = 0: indices [1, 3] → y_child = [0, 0] → entropy = 0.0\n",
    "        - When x = 1: indices [0, 2, 4] → y_child = [1, 1, 1] → entropy = 0.0\n",
    "    Step 3: Calculate weighted child entropy\n",
    "        - Weight for x=0: 2/5 = 0.4, entropy = 0.0\n",
    "        - Weight for x=1: 3/5 = 0.6, entropy = 0.0\n",
    "        - Weighted entropy = 0.4 * 0.0 + 0.6 * 0.0 = 0.0\n",
    "    Step 4: Calculate information gain\n",
    "        - Info gain = 0.972 - 0.0 = 0.972\n",
    "    Output: 0.972 (perfect split!)\n",
    "    \"\"\"\n",
    "    parent_entropy = entropy(y) # getting the parent entropy\n",
    "    values = np.unique(x) # finding the unique values to split from X\n",
    "    weighted_entropy = 0\n",
    "    for v in values:\n",
    "        y_child = y[v == x] # getting the coresponding value of x which is v in y\n",
    "        weighted_entropy += (len(y_child) / len(y)) * entropy(y_child) #using the formula to get the weighted entropy of child\n",
    "\n",
    "    return parent_entropy - weighted_entropy # substracting weighted entropy of child from parent entropy to get the information gain\n",
    "     \n",
    "\n",
    "def best_split(x, y):\n",
    "    \"\"\"\n",
    "    Determines the index of the feature that provides the highest information gain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Feature matrix of shape (n_samples, n_features).\n",
    "    y : np.ndarray\n",
    "        Target labels of shape (n_samples,).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Index of the feature that gives the maximum information gain.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Calls `info_gain` on each feature (column) of x.\n",
    "    - Returns the feature index corresponding to the largest gain.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Input: x = [[1,1], [0,1], [1,0], [0,0], [1,1]], y = [1, 0, 1, 0, 1]\n",
    "    Step 1: Calculate info gain for each feature\n",
    "        - Feature 0: x[:,0] = [1, 0, 1, 0, 1]\n",
    "          → info_gain([1, 0, 1, 0, 1], [1, 0, 1, 0, 1]) = 0.972\n",
    "        - Feature 1: x[:,1] = [1, 1, 0, 0, 1]\n",
    "          → info_gain([1, 1, 0, 0, 1], [1, 0, 1, 0, 1]) = 0.019\n",
    "    Step 2: Find maximum gain\n",
    "        - gains = [0.972, 0.019]\n",
    "        - argmax(gains) = 0\n",
    "    Output: 0 (Feature 0 provides the best split)\n",
    "    \"\"\"\n",
    "    gains = [info_gain(x[:, i], y) for i in range(x.shape[1])]   # sending all values of single feature and whole y to get the info gain\n",
    "    return np.argmax(gains) # returns the index of the largest gain\n",
    "\n",
    "def build_tree(x, y):\n",
    "    \"\"\"\n",
    "    Recursively builds decision tree using information gain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Feature matrix of shape (n_samples, n_features), where each row is a sample and each column is a feature.\n",
    "    y : np.ndarray\n",
    "        Target labels of the samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict or int\n",
    "        A nested dict representing the decision tree or an integer (0, 1) if a node is a leaf.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - If all labels in y are the same, returns that label.\n",
    "    - If no features left to split, returns the majority class.\n",
    "    - Recursively splits the data based on the feature with the highest info gain.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Input: x = [[1,1], [0,1], [1,0], [0,0], [1,1]], y = [1, 0, 1, 0, 1]\n",
    "    Step 1: Check stopping conditions\n",
    "        - len(np.unique(y)) = 2 (not pure, continue)\n",
    "        - x.shape[1] = 2 (features available, continue)\n",
    "    Step 2: Find best split\n",
    "        - best_split(x, y) returns feature_idx = 0\n",
    "    Step 3: Create node and split data\n",
    "        - Node: {\"feature\": 0, \"branches\": {}}\n",
    "        - Split by feature 0 values: [0, 1]\n",
    "    Step 4: Recursive calls for each branch\n",
    "        - Branch 0 (x[feature_0] == 0):\n",
    "          → x_subset = [[0,1], [0,0]], y_subset = [0, 0]\n",
    "          → All labels same → returns 0\n",
    "        - Branch 1 (x[feature_0] == 1):\n",
    "          → x_subset = [[1,1], [1,0], [1,1]], y_subset = [1, 1, 1]\n",
    "          → All labels same → returns 1\n",
    "    Output: {\"feature\": 0, \"branches\": {0: 0, 1: 1}}\n",
    "    \"\"\"\n",
    "\n",
    "    # if all the lables in the y are same, then its a leaf node. return it.\n",
    "    if len(np.unique(y)) == 1:\n",
    "        return int(y[0])\n",
    "    \n",
    "\n",
    "    # if there's no feature left to split, then return the majority class\n",
    "    if x.shape[1] == 0:\n",
    "        return int(np.round(np.mean(y)))\n",
    "\n",
    "    feature_idx = best_split(x, y) # getting the best feature to split \n",
    "\n",
    "    tree = {\"feature\": feature_idx, \"branches\": {}} #creating a node\n",
    "\n",
    "    for value in np.unique(x[:, feature_idx]):  # looping through number of values in the splitted feature\n",
    "        idx = X[:, feature_idx] == value   # masking to get the index of splitted values \n",
    "        subtree = build_tree(x[idx], y[idx]) # calling build_tree with the splitted values\n",
    "        tree[\"branches\"][int(value)] = subtree\n",
    "    \n",
    "    return tree\n",
    "\n",
    "def predict(tree, sample):\n",
    "    \"\"\"\n",
    "    Makes a prediction for a single sample using the trained decision tree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : dict or int\n",
    "        The decision tree structure returned by build_tree(). Can be:\n",
    "        - dict: Internal node with 'feature' and 'branches' keys\n",
    "        - int: Leaf node containing the class prediction (0 or 1)\n",
    "    sample : np.ndarray or list\n",
    "        A single sample with feature values to predict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Predicted class label (0 or 1).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Recursively traverses the tree based on feature values in the sample.\n",
    "    - At each internal node, uses the sample's feature value to decide which branch to follow.\n",
    "    - Returns the class label when reaching a leaf node.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Input: tree = {\"feature\": 0, \"branches\": {0: 0, 1: 1}}, sample = [1, 0]\n",
    "    Step 1: Check if tree is leaf\n",
    "        - isinstance(tree, int) = False (it's a dict, continue)\n",
    "    Step 2: Get feature index and sample value\n",
    "        - feature_idx = tree['feature'] = 0\n",
    "        - value = sample[0] = 1\n",
    "    Step 3: Follow the corresponding branch\n",
    "        - tree['branches'][1] = 1 (this is a leaf node)\n",
    "    Step 4: Recursive call with leaf node\n",
    "        - predict(1, sample) → isinstance(1, int) = True → return 1\n",
    "    Output: 1\n",
    "\n",
    "    Another example with deeper tree:\n",
    "    Input: tree = {\"feature\": 0, \"branches\": {0: {\"feature\": 1, \"branches\": {0: 0, 1: 1}}, 1: 1}}, \n",
    "           sample = [0, 1]\n",
    "    Step 1: At root node, check feature 0\n",
    "        - sample[0] = 0 → follow branches[0]\n",
    "    Step 2: At second node, check feature 1\n",
    "        - sample[1] = 1 → follow branches[1] = 1 (leaf)\n",
    "    Step 3: Return leaf value\n",
    "    Output: 1\n",
    "    \"\"\"\n",
    "    if isinstance(tree, int):\n",
    "        return tree\n",
    "    \n",
    "    feature_idx = tree['feature']\n",
    "\n",
    "    value = sample[feature_idx]\n",
    "\n",
    "    return predict(tree['branches'][value], sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6a78a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = build_tree(X, y)\n",
    "[predict(tree, sample) for sample in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e5539e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([\n",
    "    [1, 0, 0, 0, 1, 0],  # free + short → could be spam\n",
    "    [1, 1, 0, 0, 0, 0],  # free + ! → spam\n",
    "    [0, 1, 1, 0, 0, 1],  # ! + offer + link → tricky\n",
    "    [1, 0, 1, 1, 0, 1],  # free + offer + win + link → spam\n",
    "    [0, 0, 0, 1, 1, 0],  # win + short → could be not spam\n",
    "    [0, 1, 0, 0, 1, 1],  # ! + short + link → tricky\n",
    "])\n",
    "\n",
    "\n",
    "[predict(tree, sample) for sample in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c6824a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "model = tree.DecisionTreeClassifier()\n",
    "clf = model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1aaad45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature_5 <= 0.50\n",
      "|   |--- class: 0\n",
      "|--- feature_5 >  0.50\n",
      "|   |--- class: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.predict(X_test)\n",
    "text_representation = tree.export_text(clf)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4017c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
