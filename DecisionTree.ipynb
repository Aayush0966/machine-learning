{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5694f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(\n",
    "    [\n",
    "        [1, 1, 0, 0, 1, 1],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 1, 1],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [1, 0, 0, 1, 0, 1],\n",
    "        [0, 1, 1, 0, 1, 0],\n",
    "        [1, 1, 0, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [1, 0, 1, 1, 0, 1],\n",
    "        [0, 1, 0, 0, 1, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37006b9c",
   "metadata": {},
   "source": [
    "# Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fe1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Computes the entropy of the labels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: np.ndarray\n",
    "        Array of labels. eg: [1,0,0,1,1,1,0]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Entropy value which gives the impurity of the labels\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "        - Uses the formula: H(y) = -sum(p_i * log2(p_u)) for all classes of i.\n",
    "        - Zero probs are ignored to avoid log2(0).\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Input: y = [1, 0, 1, 0, 1]\n",
    "    Step 1: Calculate probabilities\n",
    "        - Class 0: count = 2, probability = 2/5 = 0.4\n",
    "        - Class 1: count = 3, probability = 3/5 = 0.6\n",
    "    Step 2: Apply entropy formula\n",
    "        - H(y) = -(0.4 * log2(0.4) + 0.6 * log2(0.6))\n",
    "        - H(y) = -(0.4 * (-1.32) + 0.6 * (-0.74))\n",
    "        - H(y) = -(-0.528 + (-0.444)) = 0.972\n",
    "    Output: 0.972 (high entropy indicating mixed classes)\n",
    "    \"\"\"\n",
    "    probs = np.bincount(y) / len(\n",
    "        y\n",
    "    )  # bincount counts how many samples are there in each class\n",
    "    probs = probs[probs > 0]  # removing zero probs\n",
    "    return -np.sum(probs * np.log2(probs))  # applying the formula\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    probs = np.bincount(y) / len(y)\n",
    "    return 1 - np.sum(probs**2)\n",
    "\n",
    "\n",
    "def info_gain(x, y):\n",
    "    \"\"\"\n",
    "    Calculates the information gain of splitting labels y using feature x.\n",
    "\n",
    "    :param x: np.ndarray\n",
    "            Array of single feature's values (shape: n_sampels, ).\n",
    "\n",
    "    :param y: np.ndarray\n",
    "            Array of target labels (shape: n_samples, ).\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    float\n",
    "        Info gain obtained by splitting y based on feature x\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "        - Info gain = parent entropy - weighted child entropy\n",
    "        - Weighted child entropy is calculated based on the proportion of samples\n",
    "        in each unique feature value.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Input: x = [1, 0, 1, 0, 1], y = [1, 0, 1, 0, 1]\n",
    "    Step 1: Calculate parent entropy\n",
    "        - Parent entropy = entropy([1, 0, 1, 0, 1]) = 0.972\n",
    "    Step 2: Split by feature values\n",
    "        - When x = 0: indices [1, 3] → y_child = [0, 0] → entropy = 0.0\n",
    "        - When x = 1: indices [0, 2, 4] → y_child = [1, 1, 1] → entropy = 0.0\n",
    "    Step 3: Calculate weighted child entropy\n",
    "        - Weight for x=0: 2/5 = 0.4, entropy = 0.0\n",
    "        - Weight for x=1: 3/5 = 0.6, entropy = 0.0\n",
    "        - Weighted entropy = 0.4 * 0.0 + 0.6 * 0.0 = 0.0\n",
    "    Step 4: Calculate information gain\n",
    "        - Info gain = 0.972 - 0.0 = 0.972\n",
    "    Output: 0.972 (perfect split!)\n",
    "    \"\"\"\n",
    "    parent_entropy = gini(y)  # getting the parent entropy\n",
    "    values = np.unique(x)  # finding the unique values to split from X\n",
    "    weighted_entropy = 0\n",
    "    for v in values:\n",
    "        # getting the coresponding value of x which is v in y\n",
    "        y_child = y[v == x]\n",
    "        weighted_entropy += (len(y_child) / len(y)) * entropy(\n",
    "            y_child\n",
    "        )  # using the formula to get the weighted entropy of child\n",
    "\n",
    "    return (\n",
    "        parent_entropy - weighted_entropy\n",
    "    )  # substracting weighted entropy of child from parent entropy to get the information gain\n",
    "\n",
    "\n",
    "def best_split(x, y):\n",
    "    \"\"\"\n",
    "    Determines the index of the feature that provides the highest information gain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Feature matrix of shape (n_samples, n_features).\n",
    "    y : np.ndarray\n",
    "        Target labels of shape (n_samples,).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Index of the feature that gives the maximum information gain.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Calls `info_gain` on each feature (column) of x.\n",
    "    - Returns the feature index corresponding to the largest gain.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Input: x = [[1,1], [0,1], [1,0], [0,0], [1,1]], y = [1, 0, 1, 0, 1]\n",
    "    Step 1: Calculate info gain for each feature\n",
    "        - Feature 0: x[:,0] = [1, 0, 1, 0, 1]\n",
    "          → info_gain([1, 0, 1, 0, 1], [1, 0, 1, 0, 1]) = 0.972\n",
    "        - Feature 1: x[:,1] = [1, 1, 0, 0, 1]\n",
    "          → info_gain([1, 1, 0, 0, 1], [1, 0, 1, 0, 1]) = 0.019\n",
    "    Step 2: Find maximum gain\n",
    "        - gains = [0.972, 0.019]\n",
    "        - argmax(gains) = 0\n",
    "    Output: 0 (Feature 0 provides the best split)\n",
    "    \"\"\"\n",
    "    gains = [\n",
    "        info_gain(x[:, i], y) for i in range(x.shape[1])\n",
    "    ]  # sending all values of single feature and whole y to get the info gain\n",
    "    return np.argmax(gains)  # returns the index of the largest gain\n",
    "\n",
    "\n",
    "def build_tree(x, y):\n",
    "    \"\"\"\n",
    "    Recursively builds decision tree using information gain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Feature matrix of shape (n_samples, n_features), where each row is a sample and each column is a feature.\n",
    "    y : np.ndarray\n",
    "        Target labels of the samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict or int\n",
    "        A nested dict representing the decision tree or an integer (0, 1) if a node is a leaf.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - If all labels in y are the same, returns that label.\n",
    "    - If no features left to split, returns the majority class.\n",
    "    - Recursively splits the data based on the feature with the highest info gain.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Input: x = [[1,1], [0,1], [1,0], [0,0], [1,1]], y = [1, 0, 1, 0, 1]\n",
    "    Step 1: Check stopping conditions\n",
    "        - len(np.unique(y)) = 2 (not pure, continue)\n",
    "        - x.shape[1] = 2 (features available, continue)\n",
    "    Step 2: Find best split\n",
    "        - best_split(x, y) returns feature_idx = 0\n",
    "    Step 3: Create node and split data\n",
    "        - Node: {\"feature\": 0, \"branches\": {}}\n",
    "        - Split by feature 0 values: [0, 1]\n",
    "    Step 4: Recursive calls for each branch\n",
    "        - Branch 0 (x[feature_0] == 0):\n",
    "          → x_subset = [[0,1], [0,0]], y_subset = [0, 0]\n",
    "          → All labels same → returns 0\n",
    "        - Branch 1 (x[feature_0] == 1):\n",
    "          → x_subset = [[1,1], [1,0], [1,1]], y_subset = [1, 1, 1]\n",
    "          → All labels same → returns 1\n",
    "    Output: {\"feature\": 0, \"branches\": {0: 0, 1: 1}}\n",
    "    \"\"\"\n",
    "\n",
    "    # if all the lables in the y are same, then its a leaf node. return it.\n",
    "    if len(np.unique(y)) == 1:\n",
    "        return int(y[0])\n",
    "\n",
    "    # if there's no feature left to split, then return the majority class\n",
    "    if x.shape[1] == 0:\n",
    "        return int(np.round(np.mean(y)))\n",
    "\n",
    "    feature_idx = best_split(x, y)  # getting the best feature to split\n",
    "\n",
    "    tree = {\"feature\": feature_idx, \"branches\": {}}  # creating a node\n",
    "\n",
    "    for value in np.unique(\n",
    "        x[:, feature_idx]\n",
    "    ):  # looping through number of values in the splitted feature\n",
    "        # masking to get the index of splitted values\n",
    "        idx = X[:, feature_idx] == value\n",
    "        subtree = build_tree(\n",
    "            x[idx], y[idx]\n",
    "        )  # calling build_tree with the splitted values\n",
    "        tree[\"branches\"][int(value)] = subtree\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "def predict(tree, sample):\n",
    "    \"\"\"\n",
    "    Makes a prediction for a single sample using the trained decision tree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : dict or int\n",
    "        The decision tree structure returned by build_tree(). Can be:\n",
    "        - dict: Internal node with 'feature' and 'branches' keys\n",
    "        - int: Leaf node containing the class prediction (0 or 1)\n",
    "    sample : np.ndarray or list\n",
    "        A single sample with feature values to predict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Predicted class label (0 or 1).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Recursively traverses the tree based on feature values in the sample.\n",
    "    - At each internal node, uses the sample's feature value to decide which branch to follow.\n",
    "    - Returns the class label when reaching a leaf node.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Input: tree = {\"feature\": 0, \"branches\": {0: 0, 1: 1}}, sample = [1, 0]\n",
    "    Step 1: Check if tree is leaf\n",
    "        - isinstance(tree, int) = False (it's a dict, continue)\n",
    "    Step 2: Get feature index and sample value\n",
    "        - feature_idx = tree['feature'] = 0\n",
    "        - value = sample[0] = 1\n",
    "    Step 3: Follow the corresponding branch\n",
    "        - tree['branches'][1] = 1 (this is a leaf node)\n",
    "    Step 4: Recursive call with leaf node\n",
    "        - predict(1, sample) → isinstance(1, int) = True → return 1\n",
    "    Output: 1\n",
    "\n",
    "    Another example with deeper tree:\n",
    "    Input: tree = {\"feature\": 0, \"branches\": {0: {\"feature\": 1, \"branches\": {0: 0, 1: 1}}, 1: 1}},\n",
    "           sample = [0, 1]\n",
    "    Step 1: At root node, check feature 0\n",
    "        - sample[0] = 0 → follow branches[0]\n",
    "    Step 2: At second node, check feature 1\n",
    "        - sample[1] = 1 → follow branches[1] = 1 (leaf)\n",
    "    Step 3: Return leaf value\n",
    "    Output: 1\n",
    "    \"\"\"\n",
    "    if isinstance(tree, int):\n",
    "        return tree\n",
    "\n",
    "    feature_idx = tree[\"feature\"]\n",
    "\n",
    "    value = sample[feature_idx]\n",
    "\n",
    "    return predict(tree[\"branches\"][value], sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a78a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = build_tree(X, y)\n",
    "[predict(tree, sample) for sample in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e5539e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array(\n",
    "    [\n",
    "        [1, 0, 0, 0, 1, 0],  # free + short → could be spam\n",
    "        [1, 1, 0, 0, 0, 0],  # free + ! → spam\n",
    "        [0, 1, 1, 0, 0, 1],  # ! + offer + link → tricky\n",
    "        [1, 0, 1, 1, 0, 1],  # free + offer + win + link → spam\n",
    "        [0, 0, 0, 1, 1, 0],  # win + short → could be not spam\n",
    "        [0, 1, 0, 0, 1, 1],  # ! + short + link → tricky\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "[predict(tree, sample) for sample in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa20f4f",
   "metadata": {},
   "source": [
    "# Practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4017c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ac560bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self\n",
    "\n",
    "\n",
    "def gini(x):\n",
    "    classes, counts = np.unique(x, return_counts=True)\n",
    "    probs = counts / len(x)\n",
    "    return 1 - np.sum(probs**2)\n",
    "\n",
    "\n",
    "def info_gain(feature, parent):\n",
    "    parent_gain = gini(parent)\n",
    "    best_gain = -1\n",
    "    best_thresh = None\n",
    "\n",
    "    unique_values = np.unique(feature)\n",
    "    if len(unique_values) == 1:\n",
    "        return 0, unique_values[0]\n",
    "\n",
    "    thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n",
    "\n",
    "    for t in thresholds:\n",
    "        left = parent[feature <= t]\n",
    "        right = parent[feature >= t]\n",
    "        weighted_gini = (len(left) / len(parent)) * gini(left) + (\n",
    "            len(right) / len(parent)\n",
    "        ) * gini(right)\n",
    "        gain = parent_gain - weighted_gini\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_thresh = t\n",
    "\n",
    "    return best_gain, best_thresh\n",
    "\n",
    "\n",
    "def best_split(feature, parent):\n",
    "    gains = []\n",
    "    thresholds = []\n",
    "\n",
    "    for i in range(feature.shape[1]):\n",
    "        gain, thresh = info_gain(feature[:, i], parent)\n",
    "        gains.append(gain)\n",
    "        thresholds.append(thresh)\n",
    "\n",
    "    high_gain_idx = np.argmax(gains)\n",
    "    best_thresh = thresholds[high_gain_idx]\n",
    "    return high_gain_idx, best_thresh\n",
    "\n",
    "\n",
    "def build_tree(feature, parent):\n",
    "    if len(np.unique(parent)) == 1:\n",
    "        return int(parent[0])\n",
    "\n",
    "    if feature.shape[1] == 0:\n",
    "        return int(np.round(np.mean(feature)))\n",
    "\n",
    "    idx, thresh = best_split(feature, parent)\n",
    "\n",
    "    tree = {\"feature_idx\": idx, \"threshold\": thresh, \"branches\": {}}\n",
    "\n",
    "    left_mask = feature[:, idx] <= thresh\n",
    "    right_mask = feature[:, idx] > thresh\n",
    "    tree[\"branches\"][\"left\"] = build_tree(feature[left_mask], parent[left_mask])\n",
    "    tree[\"branches\"][\"right\"] = build_tree(feature[right_mask], parent[right_mask])\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "def predict(tree, sample):\n",
    "    if isinstance(tree, int):\n",
    "        return tree\n",
    "\n",
    "    idx = tree[\"feature_idx\"]\n",
    "    thresh = tree[\"threshold\"]\n",
    "\n",
    "    if sample[idx] <= thresh:\n",
    "        return predict(tree[\"branches\"][\"left\"], sample)\n",
    "    else:\n",
    "        return predict(tree[\"branches\"][\"right\"], sample)\n",
    "\n",
    "\n",
    "def predict_all(tree, X):\n",
    "    return np.array([predict(tree, X[i]) for i in range(X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6323ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split \n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e247a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77959329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9308510638297872)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict_all(tree, X_test)  \n",
    "acc = np.sum(prediction == y_test) / len(y_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4339498a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9574468085106383)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "sklearn_prediction = model.predict(X_test)\n",
    "\n",
    "np.sum(prediction == sklearn_prediction) / len(sklearn_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7bbdff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
